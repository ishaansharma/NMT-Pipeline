{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMntJaSqpXFiH705aOLk+4X"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qauuYkkLjnOG"
      },
      "source": [
        "# Clone repo, install requirements\n",
        "!python --version\n",
        "!git clone https://github.com/OpenNMT/OpenNMT-py.git\n",
        "%cd OpenNMT-py\n",
        "!python setup.py install\n",
        "!pip install -r /content/OpenNMT-py/requirements.opt.txt\n",
        "\n",
        "# Remove default data\n",
        "import shutil\n",
        "shutil.rmtree('data')\n",
        "\n",
        "# Create new data directory\n",
        "!mkdir data\n",
        "!mkdir data/src_tgt\n",
        "!mv src_tgt.yaml data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAy0oNgCWT_7"
      },
      "source": [
        "# Create corpus object\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "\n",
        "class Corpus:\n",
        "  \"\"\"\n",
        "  Represents a parallel text dataset.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, path, firstcol, secondcol):\n",
        "    self.path = path\n",
        "    self.firstcol = firstcol\n",
        "    self.secondcol = secondcol\n",
        "  \n",
        "  def validationsplit(self, src, tgt, outpath):\n",
        "    \"\"\"\n",
        "    Split corpus into train, validation, and test data files.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    src : text\n",
        "      Name of source language (left column in corpus)\n",
        "    tgt : text\n",
        "      Name of target language (right column in corpus)\n",
        "    \"\"\"\n",
        "    # Initialize DataFrame\n",
        "    self.src = src\n",
        "    self.tgt = tgt\n",
        "    self.outpath = outpath\n",
        "    self.df = df = pd.read_csv(self.path,\n",
        "                               engine='python',\n",
        "                               sep='\\t',\n",
        "                               encoding='utf-8',\n",
        "                               header=None,\n",
        "                               names=[self.firstcol,self.secondcol],\n",
        "                               quoting=csv.QUOTE_NONE,\n",
        "                               warn_bad_lines=True,\n",
        "                               error_bad_lines=False).replace('\"','',regex=True)\n",
        "\n",
        "    self.df = self.df.sample(frac=1).reset_index(drop=True).dropna()\n",
        "\n",
        "    # Generate train datasets\n",
        "    src_train = self.df[src].iloc[:-5500]\n",
        "    src_train.reset_index(inplace=True,drop=True)\n",
        "    src_train_path = os.path.join(outpath,'src-train.txt')\n",
        "    src_train.to_csv(src_train_path,header=False,index=False,encoding='utf-8')\n",
        "    with open(src_train_path,encoding='utf-8') as f:\n",
        "      src_train_len = len(f.readlines())\n",
        "\n",
        "    tgt_train = self.df[tgt].iloc[:-5500]\n",
        "    tgt_train.reset_index(inplace=True,drop=True)\n",
        "    \n",
        "    tgt_train_path = os.path.join(outpath,'tgt-train.txt')\n",
        "    tgt_train.to_csv(tgt_train_path,header=False,index=False,encoding='utf-8')\n",
        "    with open(tgt_train_path,encoding='utf-8') as f:\n",
        "      tgt_train_len = len(f.readlines())\n",
        "\n",
        "    assert src_train_len == tgt_train_len, \"Train Datasets must be equal length\"\n",
        "    print('Source training dataset created at %s (%d lines)' % (src_train_path, src_train_len))\n",
        "    print('Target training dataset created at %s (%d lines)' % (tgt_train_path, tgt_train_len))\n",
        "\n",
        "\n",
        "    # Generate validation datasets\n",
        "    src_val = self.df[src].iloc[-5500:-500]\n",
        "    src_val.reset_index(inplace=True,drop=True)\n",
        "    src_val_path = os.path.join(outpath,'src-val.txt')\n",
        "    src_val.to_csv(src_val_path,header=False,index=False,encoding='utf-8')\n",
        "    with open(src_val_path,encoding='utf-8') as f:\n",
        "      src_val_len = len(f.readlines())\n",
        "\n",
        "    tgt_val = self.df[tgt].iloc[-5500:-500]\n",
        "    tgt_val.reset_index(inplace=True,drop=True)\n",
        "    tgt_val_path = os.path.join(outpath,'tgt-val.txt')\n",
        "    tgt_val.to_csv(tgt_val_path,header=False,index=False,encoding='utf-8')\n",
        "    with open(tgt_val_path,encoding='utf-8') as f:\n",
        "      tgt_val_len = len(f.readlines())\n",
        "    \n",
        "    assert src_val_len == tgt_val_len, \"Validation Datasets must be equal length\"\n",
        "    print('Source validation dataset created at %s (%d lines)' % (src_val_path, src_val_len))\n",
        "    print('Target validation dataset created at %s (%d lines)' % (tgt_val_path, tgt_val_len))\n",
        "\n",
        "\n",
        "    # Generate test datasets\n",
        "    src_test = self.df[src].iloc[-500:]\n",
        "    src_test.reset_index(inplace=True,drop=True)\n",
        "    src_test_path = os.path.join(outpath,'src-test.txt')\n",
        "    src_test.to_csv(src_test_path,header=False,index=False,encoding='utf-8')\n",
        "    with open(src_test_path,encoding='utf-8') as f:\n",
        "      src_test_len = len(f.readlines())\n",
        "\n",
        "    tgt_test = self.df[tgt].iloc[-500:]\n",
        "    tgt_test.reset_index(inplace=True,drop=True)\n",
        "    tgt_test_path = os.path.join(outpath,'tgt-test.txt')\n",
        "    tgt_test.to_csv(tgt_test_path,header=False,index=False,encoding='utf-8')\n",
        "    with open(tgt_test_path,encoding='utf-8') as f:\n",
        "      tgt_test_len = len(f.readlines())\n",
        "\n",
        "    assert src_test_len == tgt_test_len, \"Test Datasets must be equal length\"\n",
        "    print('Source test dataset created at %s (%d lines)' % (src_test_path, src_test_len))\n",
        "    print('Target test dataset created at %s (%d lines)' % (tgt_test_path, tgt_test_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD6UTPJtbJQ_"
      },
      "source": [
        "# Instantiate a corpus object and run validation split\n",
        "data = Corpus('en_sv.txt','EN','SV')\n",
        "data.validationsplit(src='EN',tgt='SV',outpath='/content/OpenNMT-py/data/src_tgt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X14w-CR27T_k"
      },
      "source": [
        "# Train BPE\n",
        "!python tools/learn_bpe.py -i data/src_tgt/src-train.txt -o data/src_tgt/src.bpe -s 16000\n",
        "print('Trained src.bpe')\n",
        "!python tools/learn_bpe.py -i data/src_tgt/tgt-train.txt -o data/src_tgt/tgt.bpe -s 16000\n",
        "print('Trained tgt.bpe')\n",
        "\n",
        "# Apply BPE  \n",
        "!python tools/apply_bpe.py -c data/src_tgt/src.bpe -i data/src_tgt/src-train.txt -o data/src_tgt/src-train-bpe.txt\n",
        "print('BPE applied to src-train')\n",
        "!python tools/apply_bpe.py -c data/src_tgt/src.bpe -i data/src_tgt/src-val.txt -o data/src_tgt/src-val-bpe.txt\n",
        "print('BPE applied to src-val')\n",
        "!python tools/apply_bpe.py -c data/src_tgt/src.bpe -i data/src_tgt/src-test.txt -o data/src_tgt/src-test-bpe.txt\n",
        "print('BPE applied to src-test')\n",
        "!python tools/apply_bpe.py -c data/src_tgt/tgt.bpe -i data/src_tgt/tgt-train.txt -o data/src_tgt/tgt-train-bpe.txt\n",
        "print('BPE applied to tgt-train')\n",
        "!python tools/apply_bpe.py -c data/src_tgt/tgt.bpe -i data/src_tgt/tgt-val.txt -o data/src_tgt/tgt-val-bpe.txt\n",
        "print('BPE applied to tgt-val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AzFn7VRPv_f"
      },
      "source": [
        "# View GPU information before training\r\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUI3gO29F5df"
      },
      "source": [
        "# Generate vocab files\r\n",
        "!onmt_build_vocab -config data/src_tgt.yaml -n_sample 50000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSGpUf8ichGE"
      },
      "source": [
        "# Load tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs\n",
        "\n",
        "# Create train directory and begin training\n",
        "!mkdir data/src_tgt/train\n",
        "!onmt_train -config data/src_tgt.yaml -tensorboard -tensorboard_log_dir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyIiuO5HyXq4"
      },
      "source": [
        "# Run inference on test data, based on best performing model checkpoint -- here I use model_step_200000.pt for demonstration\n",
        "!onmt_translate -model data/src_tgt/train/model_step_200000.pt -src data/src_tgt/src-test.txt -output data/src_tgt/pred.txt -gpu 0 -replace_unk -verbose\n",
        "\n",
        "# Detokenize byte-pair-encoded output\n",
        "!sed -i 's/@@ //g' data/src_tgt/pred.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utw4ilYtPBx5"
      },
      "source": [
        "# Evaluate model performance using multi-bleu metric\r\n",
        "!perl tools/multi-bleu.perl data/src_tgt/tgt-test.txt < data/src_tgt/pred.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-5qHFIGPrD1"
      },
      "source": [
        "# Create model directory\n",
        "!mkdir /OpenNMT-py/data/src_tgt/models/en_sv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNn8dH_CCDfe"
      },
      "source": [
        "# Move src.bpe to model directory\n",
        "!cp /OpenNMT-py/data/src_tgt/src.bpe /OpenNMT-py/data/src_tgt/models/en_sv/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_CWRlajCIkv"
      },
      "source": [
        "# Install dependencies for model conversion\n",
        "!pip install --upgrade pip\n",
        "!pip install ctranslate2\n",
        "!pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install OpenNMT-py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97xl5GUnCJAp"
      },
      "source": [
        "# Imports\n",
        "\n",
        "# For model conversion\n",
        "import ctranslate2\n",
        "\n",
        "# For attention visualization\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# For tokenization\n",
        "import codecs\n",
        "from subword_nmt.apply_bpe import BPE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN2qC3OgrQCs"
      },
      "source": [
        "# Convert model based on saved model checkpoint -- here I use model_step_200000.pt for demonstration\n",
        "converter = ctranslate2.converters.OpenNMTPyConverter(\"/OpenNMT-py/data/src_tgt/train/model_step_200000.pt\")\n",
        "output_dir = converter.convert('/OpenNMT-py/models/en_sv', model_spec=\"TransformerBase\",force=True,quantization=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgEaKqGMLqCc"
      },
      "source": [
        "# Select translation and tokenization models\n",
        "\n",
        "def init_model(translator_path,bpe_path):\n",
        "\n",
        "  # Initialize Translator\n",
        "  translator = ctranslate2.Translator(translator_path)\n",
        "\n",
        "  # Initialize BPE model\n",
        "  mypath = codecs.open(bpe_path)\n",
        "  bpe = BPE(mypath)\n",
        "\n",
        "  return translator, bpe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXCiExK2J9vZ"
      },
      "source": [
        "translator, bpe = init_model('/content/drive/My Drive/translation/models/russian/en_ru/',\n",
        "                             OpenNMT-py/data/src_tgt/src.bpe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ2znw2yCRRp"
      },
      "source": [
        "# Run inference on cell input and return prediction with heatmap visualization of attention vectors\n",
        "\n",
        "def translate():\n",
        "  src = input('Enter input: ')\n",
        "  src_bpe = bpe.segment(src).strip()\n",
        "\n",
        "  src_tokens = src_bpe.split(' ')\n",
        "  args = translator.translate_batch([src_tokens],return_attention=True)\n",
        "  result = ' '.join(args[0][0]['tokens']).replace('@@ ','')\n",
        "\n",
        "  print('\\n')\n",
        "  print(result)\n",
        "\n",
        "  attention_vectors = args[0][0]['attention']\n",
        "  attention_xlabels = src_tokens\n",
        "  attention_ylabels = result.split(' ')\n",
        "\n",
        "  ax = sns.heatmap(attention_vectors,xticklabels=attention_xlabels,yticklabels=attention_ylabels,cmap='Blues')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4ve_EXKH6oy"
      },
      "source": [
        "# Driver\r\n",
        "\r\n",
        "translate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-ENcrjLKkyS"
      },
      "source": [
        "ValueError: 'seagreen' is not a valid value for name; supported values are 'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'crest', 'crest_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'flare', 'flare_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 'inferno', 'inferno_r', 'jet', 'jet_r', '..."
      ]
    }
  ]
}